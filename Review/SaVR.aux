\relax 
\AC@reset@newl@bel
\citation{lin2017feature}
\citation{lin2017focal}
\citation{carion2020end}
\citation{lin2014microsoft}
\citation{everingham2010pascal}
\citation{dollar2011pedestrian}
\citation{deng2009imagenet}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Difficulties and Solutions in small object detection.}}{2}{}\protected@file@percent }
\newlabel{fig 1}{{1}{2}}
\citation{aggarwal2021generative}
\citation{dosovitskiy2020image}
\citation{carion2020end}
\citation{zhu2020deformable}
\citation{beal2020toward}
\citation{rekavandi2023transformers}
\citation{wahyudi2022toward}
\citation{tong2022deep}
\citation{liu2021survey}
\citation{chen2020survey}
\citation{tong2020recent}
\citation{redmon2017yolo9000}
\citation{bochkovskiy2020yolov4}
\citation{redmon2018yolov3}
\citation{li2022yolov6}
\citation{redmon2016you}
\citation{zhang2023superyolo}
\citation{hou2022sr}
\citation{liu2016ssd}
\citation{lin2017focal}
\citation{tan2020efficientdet}
\citation{bochkovskiy2020yolov4}
\@writefile{toc}{\contentsline {section}{\numberline {II}BACKGROUND OF CNN AND TRANSFORMER}{3}{}\protected@file@percent }
\newlabel{section1.2}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}} Object Detection Models Based on CNNs}{3}{}\protected@file@percent }
\citation{tan2020efficientdet}
\citation{girshick2014rich}
\citation{ren2016faster}
\citation{he2017mask}
\citation{vaswani2017attention}
\citation{medsker2001recurrent}
\citation{dosovitskiy2020image}
\citation{carion2020end}
\citation{zhu2020deformable}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Summary several reviews related to small object detection}}{4}{}\protected@file@percent }
\newlabel{tab:addlabel}{{I}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}} Detectors based on the Transformer architecture}{4}{}\protected@file@percent }
\citation{zeng2022small}
\citation{liu2019mr}
\citation{liu2020small}
\citation{lin2017feature}
\citation{zhu2020deformable}
\citation{chen2020survey}
\citation{hu2018squeeze}
\citation{cao2022cf}
\citation{ma2021oriented}
\citation{he2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}} Hybrid architectures combining CNNs and Transformers}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}FROM CNN TO TRANSFORMER FOR SMALL OBJECT DETECTION}{5}{}\protected@file@percent }
\newlabel{section2}{{III}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Multi-Scale and Super-Resolution Techniques}{5}{}\protected@file@percent }
\citation{zeng2022small}
\citation{ren2016faster}
\citation{hu2018small}
\citation{liu2018improving}
\citation{ma2019efficient}
\citation{meng2019block}
\citation{simonyan2014very}
\citation{liang2018small}
\citation{du2018research}
\citation{liang2022cbnet}
\citation{li2022transformer}
\citation{yang2022querydet}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces [a] CNN Model Architecture. [b] Transformer Model Architecture. [c] Vit-Frcnn Model Architecture.}}{6}{}\protected@file@percent }
\newlabel{fig 1}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Classification of small object detection using CNN and Transformer architectures and popular object detection methods in each category}}{7}{}\protected@file@percent }
\newlabel{fig 1}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Application of Attention Mechanisms}{7}{}\protected@file@percent }
\citation{meng2021conditional}
\citation{wang2022anchor}
\citation{ding2023deot}
\citation{xu2022fea}
\citation{xu2023dktnet}
\citation{xu2021improved}
\citation{liu2021swin}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Three multi-scale feature methods in small object detection. Figure a employs Deformable Convolution to extract multi-level feature maps. Figure b uses a Convolutional Neural Network as the backbone to extract feature maps of various sizes for multi-scale fusion and employs an attention mechanism to achieve a natural integration of multi-scale features.Figure c and Figure d represent the ASF module and the LCA module in CF-DETR, respectively.}}{8}{}\protected@file@percent }
\newlabel{fig 1}{{4}{8}}
\citation{cao2022cf}
\citation{zeng2022nlfftnet}
\citation{lin2019ienet}
\citation{zhang2022dino}
\citation{ma2021oriented}
\citation{xi2022dycc}
\citation{sun2022multi}
\citation{xu2023dktnet}
\citation{chen2023htdet}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Utilization of Contextual Information}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Two typical models of attention mechanism application.Figure a combines spatial attention and channel attention to enhance detection accuracy.Figure b adopts deformable attention, allowing the model to dynamically select and focus on the positions with the most informative content.}}{9}{}\protected@file@percent }
\newlabel{fig 1}{{5}{9}}
\citation{singh2018sniper}
\citation{kim2018san}
\citation{bodla2017soft}
\citation{singh2018analysis}
\citation{song2021vidt}
\citation{li2022transformer}
\citation{maaz2021multi}
\citation{liang2022cbnet}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Three typical methods of context feature processing.[a] O2DETR.[b] MATR.[c] HTDet.}}{10}{}\protected@file@percent }
\newlabel{fig 1}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Improvement of Training Strategies}{10}{}\protected@file@percent }
\citation{chawla2002smote}
\citation{inoue2018data}
\citation{chawla2002smote}
\citation{aggarwal2021generative}
\citation{cubuk2018autoaugment}
\citation{ding2023sw}
\citation{zeng2022nlfftnet}
\citation{wang2021fp}
\citation{chen2022group}
\citation{zhang2022dino}
\citation{li2022dn}
\citation{zhu2021tph}
\citation{zhou2022transvod}
\citation{liu2021aerial}
\citation{hashmi2022spatio}
\citation{wang2022ptseformer}
\citation{cui2023faq}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Exploitation of Spatio-Temporal Information}{11}{}\protected@file@percent }
\citation{ma2021oriented}
\citation{meng2021conditional}
\citation{liu2022dab}
\citation{yang2022querydet}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Three typical methods of improving training strategies.[a] FP-DETR.[b] DN-DETR.[c] T-TRD.}}{12}{}\protected@file@percent }
\newlabel{fig 1}{{7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Efficiency and Optimization in Computing}{12}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Three typical methods of utilizing spatiotemporal information.Figure A simplifies the process by introducing a spatial-temporal Transformer architecture. Figure B proposes two types of small target trackers to provide a global response. Figure C presents an innovative video object detection workflow, using Sparse R-CNN combined with temporal information to generate detection objects. }}{13}{}\protected@file@percent }
\newlabel{fig 1}{{8}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Datasets}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}General image datasets}{13}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Four typical methods of optimizing computational efficiency.}}{14}{}\protected@file@percent }
\newlabel{fig 1}{{9}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Summary of Datasets}}{14}{}\protected@file@percent }
\newlabel{tab:addlabel}{{II}{14}}
\citation{yang2016wider}
\citation{goldman2019precise}
\citation{yu2022dair}
\citation{chang2019argoverse}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Datasets for Intelligent Vehicles}{15}{}\protected@file@percent }
\citation{hwang2015multispectral}
\citation{cordts2016cityscapes}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Examples of detection results on Generic Applications Dataset.}}{16}{}\protected@file@percent }
\newlabel{fig 1}{{10}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Aerial Image Datasets}{16}{}\protected@file@percent }
\citation{xia2018dota}
\citation{liu2017high}
\citation{zhu2015orientation}
\citation{mueller2016benchmark}
\citation{han2021redet}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Examples of detection results on intelligent Vehicle Domain Datasets.}}{17}{}\protected@file@percent }
\newlabel{fig 1}{{11}{17}}
\citation{yu2021active}
\citation{yang2024mm}
\citation{caesar2020nuscenes}
\citation{schumann2021radarscenes}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Examples of detection results on CQCAR-20 dataset for YOLOv5 based on CNN and DETR based on Transformer.}}{18}{}\protected@file@percent }
\newlabel{fig 1}{{12}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Active Millimeter Wave Datasets}{18}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Examples of detection results on Aerial Image Dataset.}}{19}{}\protected@file@percent }
\newlabel{fig 1}{{13}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-E}}Summary and Analysis of Datasets}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{19}{}\protected@file@percent }
\newlabel{section4}{{V}{19}}
\citation{xiaolin2022small}
\citation{zeng2022small}
\citation{law2018cornernet}
\citation{zhou2019bottom}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Examples of detection results on Aerial Image Dataset.}}{20}{}\protected@file@percent }
\newlabel{fig 1}{{14}{20}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,MyPaperReference}
\bibcite{lin2017feature}{1}
\bibcite{lin2017focal}{2}
\bibcite{carion2020end}{3}
\bibcite{lin2014microsoft}{4}
\bibcite{everingham2010pascal}{5}
\bibcite{dollar2011pedestrian}{6}
\bibcite{deng2009imagenet}{7}
\bibcite{aggarwal2021generative}{8}
\bibcite{dosovitskiy2020image}{9}
\bibcite{zhu2020deformable}{10}
\bibcite{beal2020toward}{11}
\bibcite{rekavandi2023transformers}{12}
\bibcite{wahyudi2022toward}{13}
\bibcite{tong2022deep}{14}
\bibcite{liu2021survey}{15}
\bibcite{chen2020survey}{16}
\bibcite{tong2020recent}{17}
\bibcite{redmon2017yolo9000}{18}
\bibcite{bochkovskiy2020yolov4}{19}
\bibcite{redmon2018yolov3}{20}
\bibcite{li2022yolov6}{21}
\bibcite{redmon2016you}{22}
\bibcite{zhang2023superyolo}{23}
\bibcite{hou2022sr}{24}
\bibcite{liu2016ssd}{25}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{21}{}\protected@file@percent }
\bibcite{tan2020efficientdet}{26}
\bibcite{girshick2014rich}{27}
\bibcite{ren2016faster}{28}
\bibcite{he2017mask}{29}
\bibcite{vaswani2017attention}{30}
\bibcite{medsker2001recurrent}{31}
\bibcite{zeng2022small}{32}
\bibcite{liu2019mr}{33}
\bibcite{liu2020small}{34}
\bibcite{hu2018squeeze}{35}
\bibcite{cao2022cf}{36}
\bibcite{ma2021oriented}{37}
\bibcite{he2016deep}{38}
\bibcite{hu2018small}{39}
\bibcite{liu2018improving}{40}
\bibcite{ma2019efficient}{41}
\bibcite{meng2019block}{42}
\bibcite{simonyan2014very}{43}
\bibcite{liang2018small}{44}
\bibcite{du2018research}{45}
\bibcite{liang2022cbnet}{46}
\bibcite{li2022transformer}{47}
\bibcite{yang2022querydet}{48}
\bibcite{meng2021conditional}{49}
\bibcite{wang2022anchor}{50}
\bibcite{ding2023deot}{51}
\bibcite{xu2022fea}{52}
\bibcite{xu2023dktnet}{53}
\bibcite{xu2021improved}{54}
\bibcite{liu2021swin}{55}
\bibcite{zeng2022nlfftnet}{56}
\bibcite{lin2019ienet}{57}
\bibcite{zhang2022dino}{58}
\bibcite{xi2022dycc}{59}
\bibcite{sun2022multi}{60}
\bibcite{chen2023htdet}{61}
\bibcite{singh2018sniper}{62}
\bibcite{kim2018san}{63}
\bibcite{bodla2017soft}{64}
\bibcite{singh2018analysis}{65}
\bibcite{song2021vidt}{66}
\bibcite{maaz2021multi}{67}
\bibcite{chawla2002smote}{68}
\bibcite{inoue2018data}{69}
\bibcite{cubuk2018autoaugment}{70}
\bibcite{ding2023sw}{71}
\bibcite{wang2021fp}{72}
\bibcite{chen2022group}{73}
\bibcite{li2022dn}{74}
\bibcite{zhu2021tph}{75}
\bibcite{zhou2022transvod}{76}
\bibcite{liu2021aerial}{77}
\bibcite{hashmi2022spatio}{78}
\bibcite{wang2022ptseformer}{79}
\bibcite{cui2023faq}{80}
\bibcite{liu2022dab}{81}
\bibcite{yang2016wider}{82}
\bibcite{goldman2019precise}{83}
\bibcite{yu2022dair}{84}
\bibcite{chang2019argoverse}{85}
\bibcite{hwang2015multispectral}{86}
\bibcite{cordts2016cityscapes}{87}
\bibcite{xia2018dota}{88}
\bibcite{liu2017high}{89}
\bibcite{zhu2015orientation}{90}
\bibcite{mueller2016benchmark}{91}
\bibcite{han2021redet}{92}
\bibcite{yu2021active}{93}
\bibcite{yang2024mm}{94}
\bibcite{caesar2020nuscenes}{95}
\bibcite{schumann2021radarscenes}{96}
\bibcite{xiaolin2022small}{97}
\bibcite{law2018cornernet}{98}
\bibcite{zhou2019bottom}{99}
\@writefile{toc}{\contentsline {section}{Biographies}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Ke Wang}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Yang Chen}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Sheng Li}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Jianbo Lu}{24}{}\protected@file@percent }
\gdef \@abspage@last{24}
